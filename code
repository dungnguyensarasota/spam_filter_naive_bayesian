#This builds spam filter based on Naive Bayesian algorithm
#dataset was put together by 
#Tiago A. Almeida and José María Gómez Hidalgo 
#Download from The UCI Machine Learning Repository
#Dung Nguyen 11/2020
import pandas as pd
import re
##read data
spam = pd.read_csv(r"C:\Users\19413\Desktop\SMSSpamCollection", sep='\t', header = None, 
                   names=['Label', 'SMS'])
#randomize and split train/test sample
spam = spam.sample(frac = 1, random_state = 1)
#80% of the set for training 
train_index = 4458 
train = spam.iloc[0:train_index,].reset_index()
test = spam.iloc[train_index:,].reset_index()
train["SMS"] = train["SMS"].apply(lambda x:re.sub('\W', ' ', x ))
train["SMS"] = train["SMS"].str.lower()
train["SMS"] = train["SMS"].str.split()
#build vocabulary list
full_vocabulary = []
for sms in train["SMS"]:
    for w in sms:
        full_vocabulary.append(w)
#remove duplicate words
vocabulary = set(full_vocabulary)
vocabulary = list(vocabulary)

# #start a vocabulary dictionary
word_counts_per_sms = {unique_word: [0] * len(train['SMS']) 
                       for unique_word in vocabulary}
# print(word_counts_per_sms)
for index, sms in enumerate(train['SMS']):
    for word in sms:
        word_counts_per_sms[word][index] += 1

# print("DONE")
word_counts = pd.DataFrame(word_counts_per_sms)
training_set_clean = pd.concat([train, word_counts], axis=1)
training_set_clean.head()
word_counts = pd.DataFrame(word_counts_per_sms)
training_set_clean = pd.concat([train, word_counts], axis=1)
training_set_clean.head()
x = training_set_clean["Label"].value_counts(normalize = True)

# Isolating spam and ham messages first
spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']
ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']

# P(Spam) and P(Ham)
p_spam = len(spam_messages) / len(training_set_clean)
p_ham = len(ham_messages) / len(training_set_clean)
print(p_spam,p_ham)

# N_Spam
n_words_per_spam_message = spam_messages['SMS'].apply(len)
n_spam = n_words_per_spam_message.sum()

# N_Ham
n_words_per_ham_message = ham_messages['SMS'].apply(len)
n_ham = n_words_per_ham_message.sum()

# N_Vocabulary
n_voca = len(vocabulary)

# Laplace smoothing
alpha = 1
spam_dict = {unique_word:0 for unique_word in vocabulary}
ham_dict = {unique_word:0 for unique_word in vocabulary}

for w in vocabulary:
    spam_count = spam_messages[w].sum()
    spam_prob = (spam_count + alpha)/(n_spam + alpha * n_voca)
    spam_dict[w] = spam_prob
    ham_count = ham_messages[w].sum()
    ham_prob = (ham_count + alpha)/(n_ham + alpha*n_voca)
    ham_dict[w] = ham_prob

# print(ham_dict)
import re

def classify(message):

    message = re.sub('\W', ' ', message)
    message = message.lower()
    message = message.split()

 
    #This is where we calculate:
    p_spam_given_message = p_spam
    p_ham_given_message = p_ham
    for w in message:
        if w in spam_dict.keys():
            p_spam_given_message *= spam_dict[w]
        if w in non_spam_dict.keys():
            p_ham_given_message *= ham_dict[w]
#     print('P(Spam|message):', p_spam_given_message)
#     print('P(Ham|message):', p_ham_given_message)

#     if p_ham_given_message > p_spam_given_message:
#         print('Label: Ham')
#     elif p_ham_given_message < p_spam_given_message:
#         print('Label: Spam')
#     else:
#         print('Equal proabilities, have a human classify this!')
    if p_ham_given_message > p_spam_given_message:
        return 'ham'
    elif p_spam_given_message > p_ham_given_message:
        return 'spam'
    else:
        return 'needs human classification'
m1 = 'WINNER!! This is the secret code to unlock the money: C3421.'
m2 = "Sounds good, Tom, then see u there"
test['predicted'] = test['SMS'].apply(classify)
correct = 0 
total = len(test["SMS"])
for index, row in test.iterrows():
    if row["Label"] == row["predicted"]:
        correct += 1

print('Correct:', correct)
print('Incorrect:', total - correct)
print('Accuracy:', correct/total)
